{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zYMnuUcIi7V"
      },
      "source": [
        "#Import the libraries\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('fivethirtyeight')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVXiFUTkIl0u",
        "outputId": "74963984-e66d-4303-a698-282bcde07235"
      },
      "source": [
        "#Create a 2-D feature NumPy array with random integers\n",
        "\n",
        "features = (np.random.randint(10, size=(100, 1)))\n",
        "print(features.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYqpMZqZIrbj",
        "outputId": "a02dd0c3-a076-4710-c300-a5721c940b17"
      },
      "source": [
        "# Split the dataset into 75/25 for train and test.\n",
        "\n",
        "training_dataset_length = math.ceil(len(features) * .75)\n",
        "print(training_dataset_length)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3wIjBpYIxEo"
      },
      "source": [
        "Preprocess the data i.e feature scaling to scale the data to be valued in between 0 and 1, which is a good practice to scale the data before feeding it into a neural network for optimal performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9wKWLNXIuFk"
      },
      "source": [
        "#Scale the all of the data to be values between 0 and 1 \n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1)) \n",
        "scaled_data = scaler.fit_transform(features)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgSX4WSLI71k"
      },
      "source": [
        "Here we predict the 11th values using [1,2,….,10] values and so on. \n",
        "\n",
        "Here **N = 100** and size of the sliding window is l = 10. \n",
        "\n",
        "So **x_train** will contain values of sliding windows of l = 10 and y_train will contain values of every l+1 value which we want to predict."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbtEWOriIzx-"
      },
      "source": [
        "train_data = scaled_data[0:training_dataset_length  , : ]\n",
        "\n",
        "#Splitting the data\n",
        "x_train=[]\n",
        "y_train = []\n",
        "\n",
        "for i in range(10, len(train_data)):\n",
        "    x_train.append(train_data[i-10:i,0])\n",
        "    y_train.append(train_data[i,0])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9hJZe5TJJnu"
      },
      "source": [
        "Then converting the x_train and y_train into NumPy array values and reshaping it into a 3-D array, shape accepted by the LSTM model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rhd1D5Q2JFsg"
      },
      "source": [
        "#Convert to numpy arrays\n",
        "x_train, y_train = np.array(x_train), np.array(y_train)\n",
        "\n",
        "#Reshape the data into 3-D array\n",
        "x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aywEkZ6-J58R"
      },
      "source": [
        "Build the architecture\n",
        "\n",
        "\n",
        "* Make an object of the sequential model. Then add the LSTM layer with parameters (units: the dimension of output space, input_shape: the shape of the training set, return_sequences: Ture or False, determines whether to return the last output in the output sequence or the full sequence.\n",
        "\n",
        "* We add 4 of the LSTM layers each with a dropout layer of value(0.2).\n",
        "\n",
        "{Droupout layer is a type of regularization technique which is used to prevent overfitting, but it may also increase training time in some cases.}\n",
        "\n",
        "* Final layer is the output layer which is a fully connected dense layer(units = 1, as we are predicting only one value i.e l+1).\n",
        "\n",
        "{Dense layer performs the operation on the input layers and returns the output and every neuron at the previous layer is connected to the neurons in the next layer hence it is called fully connected Dense layer.}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56RQlf-3JwD8"
      },
      "source": [
        "from keras.layers import Dropout\n",
        "\n",
        "# Initialising the RNN\n",
        "model = Sequential()\n",
        "\n",
        "model.add(LSTM(units = 50, return_sequences = True, input_shape = (x_train.shape[1], 1)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Adding a second LSTM layer and Dropout layer\n",
        "model.add(LSTM(units = 50, return_sequences = True))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Adding a third LSTM layer and Dropout layer\n",
        "model.add(LSTM(units = 50, return_sequences = True))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Adding a fourth LSTM layer and and Dropout layer\n",
        "model.add(LSTM(units = 50))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Adding the output layer\n",
        "# For Full connection layer we use dense\n",
        "# As the output is 1D so we use unit=1\n",
        "model.add(Dense(units = 1))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApHfO-KdKSip"
      },
      "source": [
        "Compile the model using ‘adam optimizer’ (It is a learning rate optimization algorithm used while training of DNN models) and error is calculated by loss function ‘mean squared error’ ( as it is a regression problem so we use mean squared error loss function).\n",
        "\n",
        "\n",
        "Then fit the model on 30 epoch(epochs are the number of times we pass the data into the neural network) and a batch size of 50(we pass the data in batches, segmenting the data into smaller parts so as for network to process the data in parts)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOHv-tv_KO9Y",
        "outputId": "cf354f34-1013-4425-8560-846fc65d12ce"
      },
      "source": [
        "#compile and fit the model on 30 epochs\n",
        "\n",
        "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
        "model.fit(x_train, y_train, epochs = 30, batch_size = 50)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "2/2 [==============================] - 6s 23ms/step - loss: 0.4324\n",
            "Epoch 2/30\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.3716\n",
            "Epoch 3/30\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.3014\n",
            "Epoch 4/30\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.2210\n",
            "Epoch 5/30\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1341\n",
            "Epoch 6/30\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1263\n",
            "Epoch 7/30\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1812\n",
            "Epoch 8/30\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1508\n",
            "Epoch 9/30\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1145\n",
            "Epoch 10/30\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1092\n",
            "Epoch 11/30\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1264\n",
            "Epoch 12/30\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1227\n",
            "Epoch 13/30\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.1205\n",
            "Epoch 14/30\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1094\n",
            "Epoch 15/30\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1264\n",
            "Epoch 16/30\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1124\n",
            "Epoch 17/30\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1131\n",
            "Epoch 18/30\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.1025\n",
            "Epoch 19/30\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1131\n",
            "Epoch 20/30\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1129\n",
            "Epoch 21/30\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1089\n",
            "Epoch 22/30\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1154\n",
            "Epoch 23/30\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1095\n",
            "Epoch 24/30\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1135\n",
            "Epoch 25/30\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1076\n",
            "Epoch 26/30\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1130\n",
            "Epoch 27/30\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1006\n",
            "Epoch 28/30\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1115\n",
            "Epoch 29/30\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1085\n",
            "Epoch 30/30\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1111\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f00aa76dad0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Prf2XWlKZnF"
      },
      "source": [
        "Create test data similar to train data, convert to NumPy array and reshape the array to 3-D shape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v606Jx2aKWbW"
      },
      "source": [
        "#Test data set\n",
        "test_data = scaled_data[training_dataset_length - 10: , : ]\n",
        "\n",
        "#splitting the x_test and y_test data sets\n",
        "x_test = []\n",
        "y_test =  features[training_dataset_length : , : ] \n",
        "\n",
        "for i in range(10,len(test_data)):\n",
        "    x_test.append(test_data[i-10:i,0])\n",
        "    \n",
        "#Convert x_test to a numpy array \n",
        "x_test = np.array(x_test)\n",
        "\n",
        "#Reshape the data into 3-D array\n",
        "x_test = np.reshape(x_test, (x_test.shape[0],x_test.shape[1],1))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YMJIruCKeIS"
      },
      "source": [
        "Making the predictions and calculating the rmse score(smaller the rmse score, better the model has performed)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbR_gCvsKcQB",
        "outputId": "f952223b-a66f-4c5d-8cee-410abf4ed13a"
      },
      "source": [
        "#check predicted values\n",
        "predictions = model.predict(x_test) \n",
        "#Undo scaling\n",
        "predictions = scaler.inverse_transform(predictions)\n",
        "\n",
        "#Calculate RMSE score\n",
        "rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
        "rmse"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.6284689347641246"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mW5DbVSeKfyq"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    }
  ]
}